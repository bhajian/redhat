apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: model-as-a-service
  namespace: argocd
spec:
  generators:
    - git:
        repoURL: https://github.com/bhajian/model-as-a-service.git
        revision: HEAD
        files:
          - path: models/*.yaml
  template:
    metadata:
      name: "{{path.basename}}"
    spec:
      project: default
      destination:
        server: https://kubernetes.default.svc
        namespace: "{{nameSpace}}"
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
          - CreateNamespace=true
      sources:
        # 1) Local Helm chart that deploys namespace, secret-copy PreSync job, vLLM, Gateway, LiteLLM
        - repoURL: https://github.com/bhajian/model-as-a-service.git
          targetRevision: HEAD
          path: charts/model-as-a-service
          helm:
            valueFiles:
              - "{{path}}"   # the model file (e.g., models/llama-3_2-8b.yaml)

        # 2) InferencePool (Gateway API Inference Extension) â€” installed in same namespace
        - chart: inferencepool
          repoURL: oci://registry.k8s.io/gateway-api-inference-extension/charts
          targetRevision: v1.0.1
          helm:
            releaseName: "vllm-{{appName}}-pool"
            parameters:
              - name: provider.name
                value: "{{providerName}}"              # e.g., gke
              - name: inferencePool.modelServers.matchLabels.app
                value: "{{appLabel}}"                   # MUST match your vLLM Deployment label
              - name: inferenceExtension.monitoring.gke.enabled
                value: "true"
