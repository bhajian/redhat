# Application/namespace identity
nameSpace: llama-3-2-8b
appName: llama-3-2-8b
providerName: gke
appLabel: vllm-llama3-2-8b

# vLLM specifics
vllm:
  replicas: 2
  image: "vllm/vllm-openai:latest"
  imagePullPolicy: IfNotPresent
  resources:
    limits:
      nvidia.com/gpu: 1
      cpu: "4"
      memory: "16Gi"
    requests:
      nvidia.com/gpu: 1
      cpu: "2"
      memory: "12Gi"
  model: "meta-llama/Llama-3.2-8B-Instruct"
  servedModelName: "llama3-2-8b"
  tensorParallelSize: 1
  maxModelLen: 8192
  dtype: "auto"
  hfTokenSecretName: "hf-token"
  env:
    - name: VLLM_WORKER_MULTIPROC_METHOD
      value: "spawn"
  extraArgs:
    - "--max-num-seqs=256"
    - "--gpu-memory-utilization=0.92"

gateway:
  className: gke-l7-regional-external-managed
  port: 80
  name: http

litellm:
  image: "ghcr.io/berriai/litellm:latest"
  replicas: 1
  port: 4000
  env:
    - name: LITELLM_MODE
      value: "proxy"
    - name: LITELLM_LOG
      value: "info"
    - name: LITELLM_RATELIMIT_TPM
      value: "300000"
    - name: LITELLM_RATELIMIT_RPM
      value: "1800"

objective:
  enabled: true
  name: llama3-2-8b
  description: "Primary objective for Llama 3.2 8B"
