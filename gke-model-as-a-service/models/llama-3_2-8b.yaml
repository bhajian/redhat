# Required fields consumed by ApplicationSet template
nameSpace: llama-3-2-8b
appName: llama-3-2-8b
providerName: gke
appLabel: vllm-llama3-2-8b

# vLLM values (consumed by our Helm chart)
vllm:
  replicas: 2
  image: "vllm/vllm-openai:latest"
  imagePullPolicy: IfNotPresent

  # GPU requests/limits â€” adjust to your node type
  resources:
    limits:
      nvidia.com/gpu: 1
      cpu: "4"
      memory: "16Gi"
    requests:
      nvidia.com/gpu: 1
      cpu: "2"
      memory: "12Gi"

  # Model + runtime args
  model: "meta-llama/Llama-3.2-8B-Instruct"
  servedModelName: "llama3-2-8b"
  tensorParallelSize: 1
  maxModelLen: 8192
  dtype: "auto"
  hfTokenSecretName: "hf-token"           # create this Secret in the namespace with key "HF_TOKEN"
  env:
    - name: HUGGING_FACE_HUB_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-token
          key: HF_TOKEN
  extraArgs:
    - "--enable-auto-tool-choice"
    - "--max-num-seqs=256"
    - "--gpu-memory-utilization=0.92"

# Inference Gateway (external HTTP)
gateway:
  className: gke-l7-regional-external-managed
  port: 80
  name: http

# LiteLLM settings
litellm:
  image: "ghcr.io/berriai/litellm:latest"
  replicas: 1
  port: 4000
  # Simple in-memory rate-limits; you can point this at Redis later
  env:
    - name: LITELLM_MODE
      value: "proxy"
    - name: LITELLM_LOG
      value: "info"
    - name: LITELLM_RATELIMIT_TPM
      value: "300000"
    - name: LITELLM_RATELIMIT_RPM
      value: "1800"

# Optional: InferenceObjective for traffic policies (weights/SLAs)
objective:
  enabled: true
  name: llama3-2-8b
  description: "Primary objective for llama 3.2 8B"
