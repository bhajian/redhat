apiVersion: v1
kind: Namespace
metadata:
  name: vllm
---
# Hugging Face token (optional here if you've already created it separately)
# apiVersion: v1
# kind: Secret
# metadata:
#   name: hf
#   namespace: vllm
# type: Opaque
# stringData:
#   HUGGING_FACE_HUB_TOKEN: "<PUT_YOUR_HF_TOKEN_HERE>"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-cache
  namespace: vllm
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 50Gi
  storageClassName: default
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama8b
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama8b
  template:
    metadata:
      labels:
        app: vllm-llama8b
    spec:
      # Uncomment if your registry.redhat.io needs this secret
      # imagePullSecrets:
      #   - name: rh-pull

      securityContext:
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch

      # Schedule on your GPU pool and tolerate its taints
      nodeSelector:
        kubernetes.azure.com/agentpool: "gpua10"
      tolerations:
        - key: "sku"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"

      # Fix permissions on the HF cache dir
      initContainers:
        - name: fix-cache-perms
          image: registry.access.redhat.com/ubi9/ubi-minimal:latest
          command: ["/bin/sh", "-lc"]
          args:
            - |
              mkdir -p /home/vllm/.cache \
              && chown -R 1000:1000 /home/vllm/.cache || true
          volumeMounts:
            - name: cache
              mountPath: /home/vllm/.cache
          securityContext:
            runAsUser: 0

      containers:
        - name: vllm
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.1
          command: ["vllm"]
          args:
            - "serve"
            - "RedHatAI/Llama-3.1-8B-Instruct"   # positional model
            - "--tensor-parallel-size"
            - "1"
            - "--dtype"
            - "bfloat16"
            - "--max-model-len"
            - "4096"                              # trimmed to fit A10
            - "--kv-cache-dtype"
            - "fp8"                               # big VRAM saver
            - "--gpu-memory-utilization"
            - "0.80"                              # below the 0.9 default
            - "--max-num-seqs"
            - "8"
            - "--swap-space"
            - "8"                                 # optional RAM spillover
            - "--enforce-eager"
            - "--port"
            - "8000"
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf
                  key: HUGGING_FACE_HUB_TOKEN
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: HF_HOME
              value: "/home/vllm/.cache"
            - name: TORCHINDUCTOR_CACHE_DIR
              value: "/tmp/torchinductor"
          # run as root to avoid torch.getpwuid() issue
          securityContext:
            runAsUser: 0
          resources:
            requests:
              nvidia.com/gpu: 1
            limits:
              nvidia.com/gpu: 1
          ports:
            - name: http
              containerPort: 8000
          volumeMounts:
            - name: cache
              mountPath: /home/vllm/.cache
          readinessProbe:
            httpGet:
              path: /v1/models
              port: http
            initialDelaySeconds: 20
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30
      volumes:
        - name: cache
          persistentVolumeClaim:
            claimName: vllm-cache
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama8b
  namespace: vllm
spec:
  type: LoadBalancer
  selector:
    app: vllm-llama8b
  ports:
    - name: http
      port: 8000
      targetPort: http
