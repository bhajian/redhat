apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama3
  template:
    metadata:
      labels:
        app: vllm-llama3
    spec:
      securityContext:
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
      imagePullSecrets:
        - name: rh-pull
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-l4
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Equal"
          value: "present"
          effect: "NoSchedule"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: topology.kubernetes.io/zone
                    operator: In
                    values: ["us-central1-a"]
      initContainers:
        - name: fix-cache-perms
          image: registry.access.redhat.com/ubi9/ubi-minimal:latest
          command: ["/bin/sh", "-c", "mkdir -p /home/vllm/.cache && chown -R 1000:1000 /home/vllm/.cache"]
          volumeMounts:
            - name: cache
              mountPath: /home/vllm/.cache
          securityContext:
            runAsUser: 0
      containers:
        - name: vllm
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3
          args:
            - "--model"
            - "meta-llama/Meta-Llama-3-8B-Instruct"
            - "--tensor-parallel-size"
            - "1"
            - "--port"
            - "8000"
            - "--host"
            - "0.0.0.0"
            - "--gpu-memory-utilization"
            - "0.90"
            - "--download-dir"
            - "/home/vllm/.cache"
            - "--served-model-name"
            - "llama3-8b"
          env:
            - name: VLLM_LOGGING_LEVEL
              value: "DEBUG"
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-secret
                  key: HUGGING_FACE_HUB_TOKEN
            - name: HF_HOME
              value: "/home/vllm/.cache"
            # critical so NVML is found in this image
            - name: LD_LIBRARY_PATH
              value: "/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/cuda/lib64:/usr/lib64:/usr/lib:/lib64:/lib"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
          securityContext:
            runAsUser: 0
          resources:
            requests:
              nvidia.com/gpu: 1
              cpu: "2"
              memory: "12Gi"
            limits:
              nvidia.com/gpu: 1
              cpu: "6"
              memory: "24Gi"
          ports:
            - name: http
              containerPort: 8000
          volumeMounts:
            - name: cache
              mountPath: /home/vllm/.cache
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30
      volumes:
        - name: cache
          persistentVolumeClaim:
            claimName: vllm-cache
